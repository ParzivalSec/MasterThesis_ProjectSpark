% Implementation
\chapter{Submodule implementations}

In order to test the viability and usefulness of Rust in the field of game engine development, the author implemented three common submodules of an engine. Because measurements of the Rust implementations alone would not be enough to tell how well or badly Rust performs in certain disciplines, all modules were also implemented in C++, serves as a baseline of the Rust module benchmarks. The systems that were selected for implementation were based on their importance and impact onto the performance of an engine and its tool suite. The first module that was implemented is a memory management system featuring rather low-level custom allocators, debugging facilities and an abstraction type that allows for combining them for more ergonomic and flexible use. The second one includes three different types of containers usable by an engine or its tools, showing how Rust handles mid-level systems, shared across an engine's core system and tool applications. The last module that was implemented features an \ac{ECS}, testing Rust's capabilities and ergonomics in a high-level system. 

This chapter is going to describe the general architectural overview of the research engine called \textit{Spark} \footnote{Name is based on the fact that it can either turn into a wildfire spreading if Rust's performance meets expectations or stop burning if it shows that it is not a competitor of C++ in any field.}. It will also shortly give an insight into the development environment of the C++ and Rust projects and is then followed by detailed descriptions of the systems \acp{API}. This will include both, general implementation details shared among the languages and differences between them. Basic code samples showcasing use cases of the systems will highlight advantages of the implementations and pitfalls that can occur when being used wrong.

\section{Spark engine architecture}

Following the concept of separated modules the current design implements each system as a standalone library. The basis of all three modules is built by a collection of common functionality grouped inside the \textit{core} library. It includes mostly functions often used in all modules. On top of the \textit{core} library, the other three engine systems are built, every single one being a standalone library without any dependencies to their siblings. This lack of dependency grants the benefit of being able to use every single one of these modules on their own. This concept is useful in the thesis's context to ensure that every system can be measured on it's own, without suffering any penalty when a depending module was implemented in a non optimal way. In a real world example the container system may be dependent upon the memory management to allow a programmer to choose the allocators used for a collection. And then again the \ac{ECS}, because it is more high-level and therefore it is allowed to depend onto lower level modules, could be built on top of the container library to use already implemented collections for holding components. But due to the need for independent measurements the author decided to follow the architecture that can be seen in Figure \ref{fig:spark_arch}.

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/spark_arch.png}
	\caption{Layers of the module architecture implemented during this thesis. Higher layer can depend onto lower ones.}
	\label{fig:spark_arch}
\end{figure}

As it can be seen in the illustration above, the application used for benchmarking the different modules is built on top of them. This architecture is the same for both projects. Beside the shown components every project also contains a test suite, containing unit-tests for the features of all systems. How the tests are written and why the author decided to implement them additionally to the systems is described in the next section.

\section{Development process}

After the global architecture of the implementation was described this sections gives an insight into the development process of it. During the whole implementation period the author followed a test-driven approach for implementing features. Without going into much detail right now, it can be said that for every feature created there were several test cases developed beforehand describing the preferred usage and expected behavior of the \ac{API}. Exact details of this are described in section \ref{tdd}, which also describes what frameworks were used and what benefit can be gained by implementing tests. Because Rust is rather young and so is the \ac{IDE} world around it, the first part of this section will shortly explain which tools were used and how they compare to the ones used in C++, a mature language with a well-grown tooling landscape.

\subsubsection{Spark++ - the C++ project}

With Windows as the development \ac{OS} the straight forward \ac{IDE} choice for the C++ project was Visual Studio 2017. Because it is planned to develop the systems for multiple platforms in the future, \textit{premake} was used to create \ac{IDE} projects for the different platforms. \textit{Premake} is a build configuration system used for generating project files for Visual Studio, XCode or other \acp{IDE}. When creating configuration files with premake, one uses \textit{Lua}, a well-known scripting language in the gaming industry. An excerpt of the build configuration used for the C++ project can be seen in Listing \ref{lst:premake5_conf}.\\

\begin{lstlisting}[caption={Part of the premake.lua file used to generate the Spark C++ project files}, label={lst:premake5_conf}, language={[5.0]Lua}]
project "MemorySystem"
kind "StaticLib"
language "C++"
targetname "mem-sys"
targetdir "build/mem_sys/%{cfg.buildcfg}"

files { "src/MemorySystem/**.h", "src/MemorySystem/**.cpp" }

links { "Core" }
includedirs { "src/Core/"}

filter "configurations:Debug"
symbols "On"
flags { "StaticRuntime" }

filter "configurations:Release"
symbols "Off"
flags { "StaticRuntime" }
\end{lstlisting}

\noindent
The above script shows how the project for the memory system is setup. The description defines what kind of binary out shall be created and the most common one can be an executable, a static or dynamic library. A project description also contains binary dependencies and needed include files, as well as a location for all files that are part of the generated project. By using filter it is also possible to define certain settings that only take effect in specific build configurations. In the context of this thesis premake was only used to generate the Visual Studio solution but simplifies the process of porting the systems to other platforms. While \textit{premake} was only used for the C++ part of the implementation, the Rust one is based upon \textit{Cargo} and its setup is described in the next section.

\subsubsection{RustySpark - the Rust project}

Before the author settled for Visual Studio Code as the \ac{IDE} for Rust, there was an attempt on using Visual Studio too. But after a view hours of trying to work with the Rust plugin for it, developed by parts of the Piston engine community, the author decided to switch to \textit{VS Code}. That decision was based upon experiences of other Rust users and due to the author's experience with the Rust plugin. When using \textit{VS Code} a programmer benefits from useful extensions for Rust development and the usage of a tool called \textit{Racer}. Racer can be found at \url{https://github.com/racer-rust/racer} and is a utility for Rust code completion, boosting productivity and helps Rust newcomers. The build process was then totally done via \textit{Cargo's} exposed functionality. 

\subsubsection{Test-driven development} \label{tdd}

Another important part of the development process, apart tooling that supports the programmer, was the decision to follow a test-driven approach. The term \textit{test-driven development} in the context of the subsystem implementation describes the process of defining the wanted behavior in test cases and then implement functionality to pass these tests. For every feature several unit-tests were implemented to ensure that the behavior of small, separated parts of the software meet expectations. Beside the benefit of a structured development process another advantage of a well-maintained test-suite is the possibility to catch regressions introduced when altering or refactoring code. This safe-guard proved useful more than once during the development process and ensured that the behavior did not change when refactoring code or moving reusable functionality into the core library. The tests for the C++ project were implemented using the \textit{Google Test Framework} that proved to be quite easy to integrate while providing direct integration into Visual Studio. The tests of the Rust project where implemented directly in the different module files and were executed via the \texttt{cargo test} command. The fact that Rust projects come with an unit-test runner out-of-the-box is a great benefit for productivity and maintainability of code. 

\section{Memory Management} \label{mem_impl}

The first implemented submodule that is going to be described is the memory management system. As it was already mentioned in section \ref{mem_theory}, an engine's memory management system is responsible to provide control over how resources are used at runtime. It tries to reach peek performance by assuming access patterns and providing different allocation strategies. Beside that is important to provide debugging facilities, especially when working with pointers and raw memory. This section will first describe the general \ac{API} and from which parts the module is composed of. It will then highlight some interesting implementation details of both languages with code samples.

The design decisions for parts of the memory system are based on resources hosted at \textit{Stefan Reinalter's} blog \textit{Molecular Musings} \cite{MOL_MS_1} \cite{MOL_MS_2} \cite{MOL_MS_3} \cite{MOL_MS_4} \cite{MOL_MS_5} and on allocation strategies described in the book \textit{Game Engine Architectures} \cite{GEA_2}. The implementation for both languages share a common \ac{API} and at least the same system members. On top of that the Rust implementation features an additional abstraction layer to showcase how a safe interface for unsafe code can be built in Rust. The system is structured into the following parts:

\begin{labeling}{bounds checker}
	\item [allocator] facility to allocate and deallocate memory - providing different kinds of allocation policies
	\item [bounds checker] debugging facility to ensure the validity of an allocated memory area
	\item [memory realm] a memory allocation facilty that combines different 
	debugging facilities with allocation policies - flexible and policy-based approach
\end{labeling}

\subsection{Virtual memory}
\blindtext

\subsection{Allocators}

The different allocation strategies implemented serve as the module's basis. In order to allow a user to store them in a polymorphic way, all of them share a common interface. This interface is implemented as a pure virtual class in C++ and as a trait in Rust. The \ac{API} all of them share is shown in Listing \ref{lst:mem_api}. Although it showcases the C++ methods, the Rust trait only differs syntactically.\\

\begin{lstlisting}[caption={Common interface among all allocator implementation. Rust trait only differs syntactically from this C++ sample.}, label={lst:mem_api}, language={C++}]
class AllocatorBase
{
public:
virtual void* Alloc(size_t size, size_t alignment, size_t offset) = 0;
virtual void Dealloc(void* memory) = 0;
virtual void Reset() = 0;
virtual size_t GetAllocationSize(void* memory) = 0;
virtual ~AllocatorBase() = 0;
};
\end{lstlisting}

\noindent
Every allocator can fulfill an allocation request, that provides size, alignment and an optional offset of the allocation.  The term \textit{alignment} is related to an address a certain object resides at in memory. A certain value is said to be aligned to X, where X is a power of two and the address is a multiple of X. Normally alignment is not a concern to the everyday programmer but in certain situations and on specific hardware explicit alignment of values can yield better performance. The offset parameter describes how much memory the allocator shall provide \textbf{before} the aligned pointer. This is an important capability when an allocator is combined with a debugging facility that needs to store meta information in front of the aligned user pointer. 

The first allocation policy, that is also the most simple one, is a \textit{linear allocator}. It provides allocations with nearly zero overhead by not allowing the deallocation of single memory blocks. When using a linear allocator, the only possibility to free allocated blocks is to reset the whole buffer. Because of this fact it is not necessary to store the order of allocations and additional meta data needed when the internal pointer has to be rearranged on deallocations. Whenever an allocation is issue to the linear allocator, internally it ensures alignment and offset requirements and then bumps a pointer, pointing to the next free space. The only meta information that needs to be stored by the allocator is a 32-bit unsigned integer, storing the allocation's size. This is needed to properly work when used in the memory realm, which is described later. Figure \ref{fig:linear_alloc} visualizes the change in a linear allocator's internal state after an allocation request was issued.

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/linear_alloc.png}
	\caption{A linear allocator's internal state before and after an allocation request.}
	\label{fig:linear_alloc}
\end{figure}

\noindent
A common use case for such an allocation strategy is allocating per-frame temporary values that can be freed all at once with the start of the next frame. 

If there is the need to free individual allocations another low overhead allocator can be used. A \textit{stack-based allocator} issues allocation requests by advancing the internal pointer forward, but it also provides the possibility to free single allocations again. One constraint though, assuming the usage pattern for better performance, is the order in which allocations can be freed. A stack-based allocator only allows deallocations to happen in the reverse order of the allocations, meaning it adheres to a \ac{LIFO} approach. In order to set the internal pointer back to a previously done allocation, the meta data size grows and so the overhead for any allocation is slightly larger than with a linear allocator. Figure \ref{fig:stack_alloc} shows the internal state of a stack-based allocator and how it is changed after an allocation and a deallocation.

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/stack_alloc.png}
	\caption{A stack-based allocator's internal state before and after an allocation request, followed by a deallocation of a single block.}
	\label{fig:stack_alloc}
\end{figure}

\noindent
Compared to Figure \ref{fig:linear_alloc} it can be seen that the meta information in front of a user's memory block, from now on called the \textit{allocation header}, has grown. Additionally to the memory block's size it also stores an offset to the allocator's beginning, again using an 32-bit unsigned integer for it. The allocation header now introduces an overhead of 8 byte. But with this initial bit of information the internal pointer can be reset to its location before the last allocation. It is also exactly that behavior which forbids deallocation in any other order than the \ac{LIFO} one. If a block not coming from the last allocation would be freed, the allocator would create a hole and corrupt the internal pointer, preventing all other currently allocated blocks from being freed. The allocator's implementation features a second mode that runs a \ac{LIFO} check on every allocation at the cost of an additional 4 bytes overhead. If the mode is activated by using conditional compilation flags, these 4 bytes are used to store an allocation id. On every deallocation the stored id has to match the allocator wide counter or else the \ac{LIFO} rule was not respected. Common usage examples for this kind of allocation strategy include level loading, where certain values can be rolled back in reverse order after a level has finished.

An extension that can be built upon a stack-based strategy is the one of a \textit{double-ended stack allocator}. The process of allocating memory does not change compared to the underlying stack policy. But the novel feature of this approach is that the allocator can grow from both sides. Memory can be requested from either the front- or the back-end, allowing for better utilization of the allocator's space. While the per-allocation overhead does not grow, one additional pointer has to be stored in the allocator to track the current state of the back-end. Figure \ref{fig:double_stack_alloc} visualizes how such an allocator can grow and shrink from both sides and an interested reader can already guess what possible problems can occur and needs additional checks. 

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/double_stack_alloc.png}
	\caption{A double-ended stack-based allocator's internal state. Growing from both sides it needs to be ensured that the internal pointers to not overlap.}
	\label{fig:double_stack_alloc}
\end{figure}

\noindent
When growing from both side, the front pointer advances into the direction of the allocator's end while the back pointer moves to the start. The allocator has to be aware of the possible situation when the two pointer would overlap. This has to be avoided because if one advances the other one, the integrity of issued allocation cannot be ensured. A double-ended stack-allocator is from great benefit when data exists in a compressed and decompressed state. Taking the example of a game world level, the compressed chunk could be loaded by one side, while the decompression allocates memory from the other side.

The last strategy implemented is a \textit{pool allocator}, used for allocation of same-sized elements. Because allocations and deallocations could happen rather frequently when dealing with many small objects, such as particles or other volatile components, they should be fast. A pool allocator allows for allocations and deallocations in a magnitude of O(1). The big-O notation describes the magnitude of O(1) to not scale with the amount of elements stored in a collection. There is one well-known data-structure that satisfy this constraint and that is a linked list. But using a standard list would introduce both, an memory and management overhead because nodes need to allocated and all of them need to be stored somewhere. The technique used in the pool allocator is called an \textit{intrusive linked list} and it comes with no overhead but a single additional pointer. This kind of list lives inside the pooled elements, but only in the ones currently not used. This requires the pooled elements to be at least as large as a \texttt{void*} on the current platform. Listing \ref{lst:freelist_ctor} showcases how such an intrusive linked list is created into a pre-allocated block of memory.\\

\begin{lstlisting}[caption={Constructor of the FreeList class in the C++ project. It showcases how the list is created in a pre-allocated memory region.}, label={lst:freelist_ctor}, language={C++}]
sp::core::FreeList::FreeList(void* memoryBegin, void* memoryEnd, size_t chunkSize)
: m_nextChunk(nullptr)
{
const ptrdiff_t memoryBlockLength = pointerUtil::pseudo_cast<char*>(memoryEnd, 0) - pointerUtil::pseudo_cast<char*>(memoryBegin, 0);
const size_t elementCount = memoryBlockLength / chunkSize;

char* memory = pointerUtil::pseudo_cast<char*>(memoryBegin, 0);
m_nextChunk = pointerUtil::pseudo_cast<FreeList*>(memory, 0);
memory += chunkSize;

FreeList* current = m_nextChunk;
for (size_t i = 0; i < elementCount - 1; ++i)
{
current->m_nextChunk = pointerUtil::pseudo_cast<FreeList*>(memory, 0);
current = current->m_nextChunk;
memory += chunkSize;
}
current->m_nextChunk = nullptr;
}
\end{lstlisting}

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/pool_alloc.png}
	\caption{A pool allocator including the pointers of the freelist, which allows for O(1) allocation and deallocation of pooled elements.}
	\label{fig:pool_alloc}
\end{figure}

\noindent
\\
With this no overhead data structure in place a user can request an element and deallocate it in O(1). One additional detail is related to how alignment and offset parameters work with the pool allocator. When creating it, the user has to provide the maximum alignment and maximum size of one element. With this information, the allocator computes the minimal chunk size of a single block to fulfill the alignment request. Other than with all previous allocators the offset has to be provided upon construction instead of per allocation. With the minimal chunk size calculated and the offset defined constantly for every element the allocator can fulfill allocation requests with an alignment requirement up to the defined maximum. This can be done because if an element is aligned to N, it is also aligned to all power of twos smaller than N. To explain this with an example, if address A is aligned to 16 bytes, it is also aligned to 8, 4 and two and can therefore serve as the storage location for objects having such alignment requirements. Figure \ref{fig:pool_alloc} shows how the freelist works when an allocation is done and how the pointers are bend when an object is returned.

Additionally to the benefits described above a pool allocator can be enhanced and turned into a growing version. In certain situations it can be useful to have the capability to grow as soon as an allocation request fails because of exhausted memory. The C++ project includes an example implementation of a growing pool allocator that is allowed to grow until it reaches a predefined maximum size constraint. The growing behavior is implemented by using \textit{virtual memory api} provided by the core library.

Although custom allocators can boost an applications performance when used in the right situations, they have limited capabilities for discovering erroneous usage and memory bugs. Because finding such bugs can be hard without any helping utilities the implemented memory feature exposes some debugging capabilities to find common bugs. How they work and which bugs they can warn about is described in the following section.

\subsection{Bounds checker}

To harden an engine's memory system it has to prove some mechanism to prevent or at least detect memory bugs. Some of them that can rather easily be detected are memory leaks and stomps. While a memory leaks describes an allocation that was not deallocated and therefore cannot be used anymore, a stomp corrupts or overwrites memory outside of the allocation's range. The author decided to only implement a capability to detect memory stomps in that thesis due to time constraints. It was decided that more time shall be invested into the different allocators and the other subsystems instead.

A technique that is used for detecting memory stomps is called \textit{bounds checking}. One of the simple variants of it is to insert some markers, also called \textit{canaries}, before and after a user's memory block. When the user frees an allocation again, it is checked whether the inserted canaries still hold the marker values or not. If the either one of the front or end canary was corrupted, an assertion is triggered notifying the programmer that a memory stomp has occurred. There are more advanced approaches that check the integrity of all canaries at every allocation and deallocation made, which allows the for faster error detection for the cost of more computational effort. 

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/canaries.png}
	\caption{Top: an allocation wrapped by valid canaries. Bottom: a value was written into the memory, overflowing its bounds and therefore invalidating the back canary.}
	\label{fig:canaries}
\end{figure}

\noindent
The value chosen for the canaries often is one that is not commonly seen in the patterns of other allocations. In the thesis implementation each canary occupies 4 bytes in memory, being implemented by a 32-bit unsigned integer, and the stored marked value is \texttt{0xCA} in hexadecimal. Such a value is also easily detectable when debugging the application and inspecting the memory. Because using an additional bounds checker that wraps every allocation, the memory system has somehow to ensure that a memory block returned by an allocator is aligned although it also needs to provide extra space for the canaries. That's one of the situation why every allocator can take an additional \textit{offset} parameter. When an offset of X bytes is provided, the allocator ensures that there are exactly X bytes space before the aligned pointer. So if an allocation request with an offset of 4 and an alignment of 16 is issues, the allocator returns a memory block that, if offset by 4 bytes, provides an alignment of 16. Figure \ref{fig:offset_alloc} showcases how an allocation with an offset parameter can look like. Because it would be rather unergonomic to have to do this for every allocation done in the whole engine, the later described memory realm will show how to combine allocators and bounds checking to automate this step.

\clearpage

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/offset_alloc.png}
	\caption{Illustrating the structure of a memory block allocated with an offset parameter. To also have space for the second canary, the allocation size if increased by 4 bytes.}
	\label{fig:offset_alloc}
\end{figure}

\noindent
As it can be seen the requested offset bytes are right in front of the aligned pointer that is later handed to the user. Because an additional 4 bytes of space is also needed for the canary at the block's back-end, the allocation size has to be increased by 4 bytes to allow the bounds checker writing to that address. The interface for the bounds checker written in Rust can be seen in Listing \ref{lst:rust_bounds_check}.\\

\begin{lstlisting}[caption={Base trait of every bounds checker in the Rust memory system.}, label={lst:rust_bounds_check}, language={C++}]
pub trait BoundsChecker {
	unsafe fn write_canary(&self, memory: *mut u8);
	fn validate_front_canary(&self, memory: *const u8);
	fn validate_back_canary(&self, memory: *const u8);
	fn get_canary(&self) -> u32;
	fn get_canary_size(&self) -> u32;
}
\end{lstlisting}

\noindent
The same interface is used in the C++ implementation. It has to be mentioned that there are two methods for both canary locations to allow meaningful assert messages when a corrupted value was detected. The next section is going to show a method for combining both every allocation strategy with every implemented bounds checking variant in a flexible and extensible way.

\subsection{Memory realm}

With allocators and bounds checking facilities in place, the memory system needs a way to expose them to a user in an ergonomically way. Although the different parts can already be used without any further abstraction, that way would be rather unergonomic for a programmer to use. In order to expose a more robust and flexible \ac{API}, the memory system contains \textit{memory realms}. This type is an abstraction put on top of allocators and bounds checking to combine them. Both, C++ and Rust, make use of generic code to implement the memory realm. Listing \ref{lst:cpp_mem_realm} shows a partial implementation lacking some details about special member functions. \\

\begin{lstlisting}[caption={Implementation of the memory realm in C++. Some implementation deatils were omitted.)}, label={lst:cpp_mem_realm}, language={C++}]
template <typename Allocator, typename BoundChecker>
class MemoryRealm : public MemoryRealmBase
{
public:
	explicit MemoryRealm(size_t bytes)
	: m_allocator(bytes)
	{}
	
	template<typename MemoryProvider>
	explicit MemoryRealm(MemoryProvider& memoryProvider)
	: m_allocator(memoryProvider.start(), memoryProvider.end())
	{}
	
	void* Alloc(size_t bytes, size_t alignment) override
	{
		const size_t totalMemory = m_boundsChecker.CANARY_SIZE + bytes + m_boundsChecker.CANARY_SIZE;
		
		char* memory = static_cast<char*>(m_allocator.Alloc(totalMemory, alignment, m_boundsChecker.CANARY_SIZE));
		
		m_boundsChecker.WriteCanary(memory);
		m_boundsChecker.WriteCanary(memory + m_boundsChecker.CANARY_SIZE + bytes);
		
		return memory + m_boundsChecker.CANARY_SIZE;
	}
	
	void Dealloc(void* memory) override
	{
		char* allocatorMemory = static_cast<char*>(memory) - m_boundsChecker.CANARY_SIZE;
		
		m_boundsChecker.ValidateFrontCanary(allocatorMemory);
		const uint32_t allocationSize = static_cast<uint32_t>(m_allocator.GetAllocationSize(allocatorMemory));
		m_boundsChecker.ValidateBackCanary(allocatorMemory + m_boundsChecker.CANARY_SIZE + allocationSize);
		
		m_allocator.Dealloc(allocatorMemory);
	}
	
	void Reset(void) override
	{
		m_allocator.Reset();
	}
	
	~MemoryRealm() = default;

private:
	Allocator m_allocator;
	BoundChecker m_boundsChecker;
};
\end{lstlisting}

\noindent
The code sample shows that a realm takes two template parameters defining the allocator and bounds checker used. An advantage of this design, also being named \textit{policy-based design}, is that each realm can be rather easily configured to use every allocator and every bounds checker, if they adhere to the same interface. To give an example, one memory realm could use a linear allocator with canary-based bounds checking during development and is exchanged by a realm using the same linear allocator but with a bounds checker that only has no-op methods. The bounds checker used in release implemented each of its methods empty, allowing the called functions to yield a no-op when being compiled for release or retail mode. With this approach a user can define memory arenas in a flexible way, creating the ones fitting a certain use case. This approach could even be enhanced, as Stefan Reinalter also describes in his blog \cite{MOL_MS_5}, by adding facilities for handling multi-threaded access and memory tracking. These parts can then also be added as template parameters and increase the possibilities a programmer has.

While the C++ implementation uses templates, the Rust approach is based upon the feature of \textit{generics}. Combined with traits the Rust implementation allows for also constraining the passed types and force them to implement a certain interface. Although that functionality would also be implementable in C++, Rust includes it in its core language, while the C++ mechanism would be a custom creation (\textit{type traits} or \textit{concepts}). In Listing \ref{lst:rust_trait_bound_basic} \& \ref{lst:rust_trait_bound_typed} it can be seen Rust's trait bounds can be useful when implementing a generic abstraction where an used type has to provide certain functionality defined by an interface.\\

\begin{lstlisting}[caption={Rust interface of the basic memory realm allowing only basic allocators.}, label={lst:rust_trait_bound_basic}, language={C++}]
pub struct BasicMemoryRealm<A: Allocator + BasicAllocator, B: BoundsChecker + Default> {
	allocator: A,
	bounds_checker: B,
}
\end{lstlisting}

\begin{lstlisting}[caption={Rust interface of the typed memory realm.}, label={lst:rust_trait_bound_typed}, language={C++}]
pub struct TypedMemoryRealm<A: Allocator + TypedAllocator, B: BoundsChecker + Default> {
allocator: A,
bounds_checker: B,
}
\end{lstlisting}

\subsection{Idiomatic Rust implementation}

While the C++ implementation returns raw pointers and indicates error states with a \texttt{nullptr}, the Rust project features a safe wrapper around allocations to also showcase an approach using idiomatic Rust principles. To allow the system to be used in safe Rust, the author needed a way to somehow make the allocation \ac{API} more robust and safe. To reach this goal, the \texttt{Box} type of the standard library served as a reference. A \texttt{Box} is a way to heap allocate in Rust and it follows the \ac{RAII} idiom as many other Rust implementations. Because the current version of the language does not feature custom allocators in a stable way, the author implement a custom type calls \texttt{AllocatorBox}. It was responsible for managing the allocated memory block and to free it properly when it is dropped. To do so it needed to hold a reference to the allocator the memory block comes from the the block itself. This also ensures, that an allocation can not live longer than the allocator it came from due to Rust's lifetime rules and the borrow checker.\\

\begin{lstlisting}[caption={AllocatorBox abstraction to allow \ac{RAII} management of allocations.}, label={lst:rust_allocator_box}, language={C++}]
pub struct AllocatorBox<'a, T: 'a + ?Sized, A: 'a + Allocator + ?Sized> {
	instance: Unique<T>,
	allocator: &'a A,
}

// ...

impl<'a, T: ?Sized, A: Allocator + ?Sized> Drop for AllocatorBox<'a, T, A> {
fn drop(&mut self) {
	unsafe {
		intrinsics::drop_in_place(self.instance.as_ptr());
		self.allocator.dealloc_raw(MemoryBlock::new(self.instance.as_ptr() as *mut u8));
	}
}

\end{lstlisting}

\noindent
Listing \ref{lst:rust_allocator_box} showcases the members of an \texttt{AllocatorBox}. It uses the \texttt{Unique} type to indicate that it is the only owner over the value and a reference to the allocator the memory block came from. At the bottom it can be seen how the \texttt{Drop} trait is implemented and what happens when an \texttt{AllocatorBox} goes out of scope. The owned instance is dropped and then a memory block is created from the owned pointer. That block is handed back to the allocator it came from, properly releasing memory as soon as it is not needed anymore. 

\newpage

\section{Containers} \label{container_impl}

With the memory system as a quite low-level one, the second part of the implementation section focuses on a slightly more high-level library featuring three different kind of containers. A container serves the purpose of holding several elements, allowing to add and access them. The author decided to implement the following three collections, all of them capable of being used at runtime or for implementing tools.

\begin{labeling}{handle map}
	\item [vector] dynamically size array, storing elements in a contiguous memory block
	\item [handle map] collection that internally stores items contiguous in memory 
	while external handles to them will be kept intact
	\item [ringbuffer] collection allowing for reading and writing to it
	with the capability of wrapping around at the end
\end{labeling}

The following sections will each describe how the collection works and how it was implemented in C++ and Rust.

\subsection{Vector}

A vector is a collection most typically found in the standard library of most programming languages. With \texttt{std::vector<T>} and \texttt{Vec<T>}, both C++ and Rust, already provide an implementation to be used by a programmer. This collection can also be described as an array that is able to grow on demand. So while it is preferable to use fixed-size arrays due to reasons such as memory waste, book-keeping overhead and performance, there are some situations where the required size of an array cannot be known at compile-time. That are the situations where a vector is the tool of choice. But why is there the need for implementing such a collection if the languages already provide one? The reason is, as often in the world of game engines, performance. Standard library collections fulfill the need of being portable and that's the reason why there are several ways how the performance of them can be improved when certain unknowns are replaced by the knowledge of a machine's capabilities and usage patterns. 

The difference between standard library implementations and the one chosen by the author is the way the collection retrieves its memory and how the growth is handled. Vectors commonly use the languages default memory allocation system to allocate space for its elements. While normally this is not a problem, it can become one as soon as the vector's current capacity is exhausted. During this situation, the vector allocates a new space that is bigger than the current one, moves all elements into that space and frees the old one. This can be either done by using allocation capabilities such as \texttt{realloc}, provided by the default allocator, or manually. Both of these approaches can lead to the problem that for a short time, the old and the new memory block are allocated at the same time, wasting memory until the old one is freed. The chosen implementation aims to avoid this problem by using the virtual memory system of the \ac{OS} to allocate and free space.

\subsubsection{Spark Vector}

Contrary to the standard libraries vectors, the one implemented for Spark only allocates its memory via the virtual memory system of the \ac{OS}. When a new one is created, it reserves virtual address space up to one \ac{GB}. Due to that fact the implementation is only useful on 64-bit platforms to the bigger address space available on them. Whenever elements are added, or pushed, to the vector, a check is performed whether it needs to grow or not. If the check results that growing the collection is necessary, physical pages meeting the required capacity are committed. And here lies the different to the standard vector implementation. While normally in this situation memory usage could nearly be doubled, Spark's vector does not need to allocate or reallocate a new block. Because all of the already reserved virtual address space lies contiguous in memory, relocation of the previously owned elements is not needed and the new elements are just appended at the end of the already committed pages.\\

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/vector_growth.png}
	\caption{Illustrating the growing process of Spark's vector.}
	\label{fig:vector_growth}
\end{figure}

\noindent
A visual description of the vector growth can be seen in Listing \ref{fig:vector_growth}. The figure shows the situation where a new element is added to an already exhausted vector. To fit the element into the collection, another range of memory pages is committed. The amount of the newly allocated capacity is determined by fixed growing rules, ranging from twice the current size to more exotic growth patterns. What exactly is done during the growing process of the vector can be seen in Listing \ref{lst:vector_growth}. This code sample shows how the \texttt{grow} method is implemented in Rust. The C++ function is now shown because they logically work in the same way only differing in syntax.\\

\begin{lstlisting}[caption={Growth function of the Spark vector in Rust.}, label={lst:vector_growth}, language={C++}]
fn grow(&mut self, bytes: usize) {
	{
		let virtual_address_space_exhausted = self.internal_array_begin.as_ptr() as *mut u8 == self.virtual_mem_end;
		debug_assert!(!virtual_address_space_exhausted, "Not enough address space to grow further");
	}
	
	let page_bytes_to_grow = math_util::round_to_next_multiple(bytes, virtual_mem::get_page_size());
	
	let is_enough_space_for_requested_pages = unsafe { self.internal_array_end.offset(page_bytes_to_grow as isize) <= self.virtual_mem_end };
	let grow_by_bytes = if is_enough_space_for_requested_pages {
		page_bytes_to_grow
	}
	else {
		let remaining_virtual_address_space = self.virtual_mem_end as usize - self.internal_array_end as usize;
		math_util::round_to_previous_multiple(remaining_virtual_address_space, virtual_mem::get_page_size())
	};
	
	let ptr = match { virtual_mem::commit_physical_memory(self.internal_array_end, grow_by_bytes) } {
		None => ptr::null_mut(),
		Some(mem) => mem,
	};
	
	if ptr.is_null() {
		debug_assert!(true, "Vector run out of memory due to an unknow error");
	}
	
	self.internal_array_end = unsafe { ptr.offset(grow_by_bytes as isize) };
	self.capacity = self.capacity + (grow_by_bytes / mem::size_of::<T>());
}
\end{lstlisting}

\noindent
While growing a vector is normally straight forward, there is a an edge case if the demanded capacity is not covered by available address space. If this situation occurs the vector does not assert or fail, but allocates all available pages until the end. That is the only case where the defined growing rule is ignore to utilize the available memory limits till the maximum. At the end, if committing the physical pages succeeded, the internal state, including current array end and capacity, is updated. It has to be mentioned that due to the virtual memory system, the vector can only grow in a multiple of the \ac{OS}'s page size.

\subsection{Handle Map} \label{handle_map_impl}

The second collection that was implemented is called a \textit{Handle Map} or a \textit{Dense-Sparse Set}. Other collections often come with the disadvantage that either elements will not be contiguous in memory after deleting some of them or pointers to them are invalidated if the collection is able to reorder them to avoid holes. This collection solves this problem by handing out ids or so called \textit{handles}. They are used to refer to some item in the collection instead of raw pointers. With that technique it is possible to reorder the internal array of items to keep them contiguous while also upholding already existing handles to them. In order to achieve this behavior the handle map contains three different internal arrays. 

One of them is the \textit{sparse array}, holding handles. A handle is simply a 64-bit unsigned integer for the user. Internally it is interpreted as an index into the sparse set (4 bytes) and a \textit{generation} part (4 bytes). The generation is important when requesting the item behind a handle to discover old references. Because handles are reused and array index can be used more than once and to ensure that no invalid or old handle can request an item, the generation of that exact handle is increased with every removing operation to deprecate all previously returned handles. 

The second array used is called the \textit{dense set} and holds objects stored by the collection. This array arranges its element to line up in memory and therefore yield better performance when iterating them due to good cache utilization. Figure \ref{fig:handle_map} shows the two array described until now and how they relate to each other.

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/handle_map.png}
	\caption{Sparse and dense array of the handle map. Relations defined by the indexes are described by arrows.}
	\label{fig:handle_map}
\end{figure}

\noindent
Whenever an item is deleted from this array, it is swapped with the last element to fill the hole and decrease the collection's size by one. But without any further processing, all handles to the previous last element would now be invalid which would mitigate all benefits that collection comes with. So whenever internal reordering occurs it is the responsibility of a handle map, to somehow bend the previously returned handles to point to the new location of the item.

For that situation the handle map contains a third array containing the reverse indexes from the dense array into the sparse array. So whenever an object is removed, the index of its handle data is looked up in that array. With that index the data related to the old handle can be accessed and the index into the dense array is overwritten by the new one. So the next time a user accesses the swapped value by its handle, the access into the sparse set will return the overwritten index, yielding the right item. With this process it is ensured that internally all elements are linear in memory while no handle is invalidated by the necessary reordering routine. 

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/handle_map_deletion.png}
	\caption{Reordering routine triggered by the deletion of item three. Ensuring old handle integrity of item seven.}
	\label{fig:handle_map_deletion}
\end{figure}

\newpage
\noindent
As it can be seen in Figure \ref{fig:handle_map_deletion}, when item three is deleted it is swapped with the last element in the dense array, being item seven in the graphic. As reaction to that situation several parts have to be updated, highlight purple in the above figure. To ensure every old handle to item seven stays valid, the previous handle data stored in the sparse array is looked up with the help of the reverse index lookup table. Then the index into the dense array stored along the received handle is bent to point to the location where item three was stored until now. The index in the reverse lookup array is also updated to signify that from now on the index two of the dense array is referred to by the handle data stored at index 6 in the sparse array. The last action that has to be taken is the increase of the generation of the handle data pointing to the removed item. So, if an old handle containing a sparse array index two and a generation lesser than one is passed to the map, an assertion signals that the value behind that handle is not valid anymore.\\

\begin{lstlisting}[caption={Deletion of an item in the C++ implementation of the handle map}, label={lst:cpp_handle_map}, language={C++}]
template <typename Item>
void HandleMap<Item>::Erase(Handle handle)
{
	const InternalId internalId = pointerUtil::pseudo_cast<InternalId>(handle, 0);
	
	{
		const bool userIndexInRange = internalId.sparseArrayIndex < m_itemCount;
		assert(userIndexInRange && "Index of handle was out of range of the handle array");
	}
	
	HandleData& handleData = m_handles[internalId.sparseArrayIndex];
	
	const bool validGeneration = internalId.generation == handleData.generation;
	if (!validGeneration)
	{
		return;
	}
	
	const uint32_t lastMeshIndex = m_itemCount - 1;
	m_items[handleData.denseArrayIndex] = m_items[lastMeshIndex];
	m_items[lastMeshIndex].~Item();
	
	const uint32_t indexOfMovedItem = m_meta[lastMeshIndex].denseToSparseIndex;
	m_handles[indexOfMovedItem].denseArrayIndex = handleData.denseArrayIndex;
	m_meta[handleData.denseArrayIndex].denseToSparseIndex = indexOfMovedItem;
	
	m_freeList.ReturnChunk(&m_handles[internalId.sparseArrayIndex]);
	
	++handleData.generation;
	m_itemCount = lastMeshIndex;
}
\end{lstlisting}

\noindent
Additionally to Figure \ref{fig:handle_map_deletion}, Listing \ref{lst:cpp_handle_map} shows how that routine is implement in C++. The Rust implementation is rather similar to the shown function. The major difference is that the Rust implementation returns an \texttt{Option} wrapping a handle. That forces the programmer to check whether the returned handle is a valid one or none, whereas the C++ implementation returns an integer that has to be checker for validity due to previously defined rules how invalid state is communicated.

\subsection{Ringbuffer}

The last collection in the contain library is called a \textit{Ringbuffer}. A ring or circular buffer is a data structure that uses an internal fixed-sized array and is able to wrap around if the writing index reaches array length. Such a buffer can either use single bytes as its elements, allowing the user to write arbitrary data, or use typed elements. The implemented variant uses fixed-size objects per ringbuffer. A common use case of that structure are producer-consumer queues where two threads, or routines in general, write and read from the same ringbuffer, using it to communicate and share data. The implemented solution contains the fixed-size array and two additional integers that hold the state of the current write and read index. Figure \ref{fig:ringbuffer} showcases the internal state of the ringbuffer in the most trivial way. The writ index is pointing to the next slot a value can be written to while the read index is indicating the next value to be read. If any index reaches the last element ad index six and wants to advance further, it wraps around starting at index zero again.

\begin{figure}[h!]
	\centering \includegraphics[width=\linewidth]{PICs/ringbuffer.png}
	\caption{Illustration of the ringbuffer showing the two indexes and the important wrap around when the end is reached.}
	\label{fig:ringbuffer}
\end{figure}


Due to the wrap around that happens as soon an index reaches the end of the array, writing to a ringbuffer can cause the oldest values contained to be overwritten. Some variants do not allow overwriting old values and yield an error or raise an exception in such a case. Both, the C++ and Rust implementation, allow for overwriting old values when the write index wraps. Another constraint the ringbuffer has to ensure is that the read index is not allowed to overtake the write index or otherwise it would read invalid or old data, running into undefined behavior in certain scenarios. The case of wrapping around is even more complicated when the buffer is used to store bytes and allow a user to insert arbitrary sized elements. It can happen that such an object is split up, having half of it stored at the end of the buffer while the other half is located at the beginning due to wrapping rules. If now the value is read again, the two parts have somehow be copied together to return the memory block previously written. One practice to simplify that case is to implement a technique known as a \textit{magic ringbuffer}. With the magical variant, the buffer is mapped to two adjacent virtual memory regions, allowing it to be read linearly and the only edge case occurs when the read pointer advances into the second virtual memory region. If that happens the read and write index is decreased by the buffer's length to be in the first sector again. Listing \ref{lst:ringbuffer_write} shows how the write function is implemented in the Rust ringbuffer and how the case of wrap around is handled.\\

\begin{lstlisting}[caption={Write function of the Rust ringbuffer showing how the wrap around case is handled}, label={lst:ringbuffer_write}, language={C++}]
pub fn write(&mut self, item: T) 
{
	self.empty = false;
	
	self.items[self.write_idx] = item;
	self.write_idx = (self.write_idx + 1) % self.capacity;
	
	if self.write_idx == self.read_idx {
		self.read_idx += 1;
	}
}
\end{lstlisting}

\noindent
One big difference between the Rust and C++ implementation is how memory for the internal array is allocated. In the C++ version, which can be seen in Listing \ref{lst:cpp_ringbuffer_interface}, the internal array is allocated on the stack because the maximum capacity is passed to the template class as a non-type template parameter. With that design there are no dynamic memory allocations involved.  As it was previously mentioned in section \ref{rust_const_generics}, Rust currently has no concept of const generics, which makes it currently not possible to provide a similar interface in Rust. Due to that fact a vector has to be used that is then initialized with the maximum capacity passed at construction of the ringbuffer. This involves at least one dynamic memory allocation at runtime that could be omitted if it would be possible to use compile-time generic parameters to create arrays in Rust.\\

\begin{lstlisting}[caption={Interface of the C++ Ringbuffer. Capacity is a non-type template parameter used to control the maximum element size.}, label={lst:cpp_ringbuffer_interface}, language={C++}]
template <typename Item, size_t Capacity>
class RingBuffer
{
	public:
	RingBuffer();
	
	void Write(const Item& element);
	void Write(const Item&& element);
	
	Item* Read(void);
	Item* Peek(void);
	void  Reset(void);
	
	bool IsEmpty(void) const { return m_isEmpty; }
	bool IsFull(void)  const { return !IsEmpty(); }
	
	size_t Size(void) const;
	
	private:
	bool m_isEmpty;
	size_t m_writeIndex;
	size_t m_readIndex;
	Item m_items[Capacity];
};
\end{lstlisting}

\section{Entity Component System} \label{ecs_impl}

The last implemented system is an \ac{ECS}. Because the performance comparison between two languages is the main goal of this thesis the author reimplemented an already existing Rust \ac{ECS} in C++. The project that served as reference is called \textit{calx-ecs} and is a rather simple variant of an \ac{ECS}. This section will describe its \ac{API} and what parts are implemented differently in C++.

\subsection{Calx ECS}

\textit{Calx-ecs} was chosen due to its simple design which allows for a quick reimplementation in another language. The project is hosted at GitHub and can be found at \url{https://github.com/rsaarelm/calx-ecs}. Although there are more popular \ac{ECS} implementations in the Rust ecosystem, this one was chosen because a full reimplementation of a more complex one would have exceeded the available amount of time for that part of the thesis. \textit{Calx-ecs} features the following parts that are described by the following paragraphs:

\begin{labeling}{component}
	\item [world] responsible for entity creation and component storage
	\item [entity] thin wrapper around two integers, an entity is only an id that groups components
	\item [component] a type that can be added to an entity, defined by the user by deriving a
	component interface (class in C++ / trait in Rust)
\end{labeling}

\subsection{Entities \& Components}

As already described in section \ref{ecs_theory} an \ac{ECS} is based upon composition instead of inheritance. While some variants implement an entity as a class containing a list of components, another one defines entities as simple ids that group components. The second approach is also the one that is used in this thesis. Every entity is a unique id stored in a 32-bit unsigned integer. Additionally to the id the \texttt{Entity} type also contains an index (also a 32-bit unsigned integer) that is used to describe where the entity is stored in the collection inside the \texttt{World} class. This allows the system to store a maximum of around 4 million entities.

Beside the \texttt{Entity} the system allows a user to define custom components which can be added and removed to an entity. The original Rust implementation defined a \texttt{Component} trait that could be implemented by user-defined types. Each component, then has to be registered with the world and a \texttt{ComponentStorage} is created per registered type. This allows for storing components contiguous in memory by using a quite similar approach then the handle map introduced in section \ref{handle_map_impl}. Listing \ref{lst:ecs_comp_storage} shows the interface of the \texttt{ComponentStorage} that is responsible to insert and removed components and keep track of which entity owns them.\\

\begin{lstlisting}[caption={ComponentStorage interface of the C++ implementation}, label={lst:ecs_comp_storage}, language={C++}]

// DenseStorage is a contiguous vector of entitites and the components of type C
template<typename Comp>
struct DenseStorage
{
	std::vector<Comp> data;
	std::vector<Entity> entities;
};

// Base class to allow storing differently typed storages in a polymorphic 
// collection
class Storage
{
	public:
	virtual void remove(const Entity& entity) = 0;
	virtual bool contains(const Entity& entity) = 0;
	virtual ~Storage() = default;
};

// The templated ComponentStorage being specialized for every user-defined
// component derived type
template<typename Comp>
class ComponentStorage : public Storage
{
	public:
	void insert(const Entity& entity, Comp& comp);
	void remove(const Entity& entity) override;
	bool contains(const Entity& entity) override;
	const std::vector<Entity>& entities(void) const;
	Comp* get(const Entity& entity);
	const Comp* get(const Entity& entity) const;
	
	private:
	DenseStorage<Comp> m_innerStorage;
	std::vector<Index> m_indexIntoInnerData;
};
\end{lstlisting}

\noindent
Due to the design of the \texttt{ComponentStorage} and the decision to use a distinct storage per component type, iterating over all components from one type is cache-friendly and yields good performance due to the linear memory layout. Because all storages and therefore all components are stored in the \texttt{World}, it is also the only way of retrieving references to single components or to query for entities owning a certain component. 

\subsection{World}
 
 The \texttt{World} is the place of the \ac{ECS} where all entities and components are managed. It is responsible for creating new entities, registering component types and managing how they are stored internally. To allow the system to work with a variable amount of user-defined types, every new component type has to be registered before an instance can be inserted. In the Rust implementation this was done by a macro which implements the \texttt{Component} trait for every specified type and adds a property of a typed \texttt{ComponentStorage} to the world type. Listing \ref{lst:rust_register_comp} shows how the invocation of the macro looks like in Rust. Because of the macro's internals and how it defines the \texttt{ECS} type, the Rust system only allows for one world, containing fixed components, to exist.\\
 
 \begin{lstlisting}[caption={Registering custom components with the Rust ECS}, label={lst:rust_register_comp}, language={C++}]
 Ecs! {
	 pos: Position,
	 vel: Velocity,
 }
 \end{lstlisting}
 
 \noindent
 Contrary to the macro approach, the reimplementation in C++ did not adhere to this approach. This decision was made due to the very different macro system of the both languages. Rust's macros create an \ac{AST} the programmer can work with and already includes features to execute part of the macro for any amount of arguments passed. While this can also be done in C++ with variadic macros it is way harder and also not as flexible. With the C++ variadic macro approach it is not possible to support any amount of arguments but only the ones defined by the user. To mirror the behavior of the Rust crate,where any number of component types can be registered, the C++ implementation also wanted to supply that possibility. Listing \ref{lst:cpp_register_comp} shows how the registration process works. Whenever a new component is added to an entity, the world checks whether there already exists a corresponding storage and if it does not then a new one is created and stored in an internal hash map. The key for this map is the unique id of the component which has to be defined by the user whenever a new component subtype is created.\\
 
 \begin{lstlisting}[caption={Method to add components, implicitly registering them on first use}, label={lst:cpp_register_comp}, language={C++}]
 template <typename C>
 void World::addComponent(const Entity& entity, C component)
 {
 	const uint32_t componentId = C::UID();
 	if (m_componentStorages.count(componentId) == 0)
 	{
 		m_componentStorages.insert({ componentId, new ComponentStorage<C>() });
 	}
 	
 	ComponentStorage<C>* storage = static_cast<ComponentStorage<C>*>(m_componentStorages.at(componentId));
 	if (contains(entity))
 	{
 		storage->insert(entity, component);
 	}
 }
\end{lstlisting}
 
\noindent
The unique id of a component used to distinguish between them is requested from a static method the type \texttt{C} has to implement. An implementation of that method can look like this: \texttt{static uint32\_t UID() \{ return 1u; \}}. It is the user's responsibility to ensure that every id returned by \texttt{UID} is unique or the system will run into undefined behavior.

\section{Conclusion}

With all the systems implemented in two languages, the author wants to shortly summarize personal experiences and opinions about Rust and the implementations. Coming from the C++ world it was at first difficult to transfer the concepts and architectural choices for the systems to Rust. With the borrow checker complaining about lifetimes and more than one mutable reference to a value, it was challenging to refactor the Rust code to even compile. But after the first few pitfalls and due to the helpful and verbose compile errors of \textit{rustc}, the author adapted a more idiomatic style and the Rust implementations become robust. It was then quite the opposite that some of the features Rust provide were missing when a C++ implementation was reworked. While Rust offers types such as \texttt{Option} for describing the state of a return value, C++ sticks with pointers and using \texttt{nullptr} to indicate an error. It can be said, in the opinion of the author, that Rust served well as a language for implementing the modules and that the built-in test facilities of cargo were helpful.

The next chapter will include the results of the performance measurements of the implemented systems. It will first describe the benchmark setup and what scenarios where chosen to compare the module to each other. After that the findings will be discussed and differences between the languages will be shown, supported by diagrams of the measurement data.


